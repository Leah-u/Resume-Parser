{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "resume_parser_to_word_cloud.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-hlLJAPEDbxW"
      },
      "source": [
        "# 1. Import libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zK8DjqOhCnd9"
      },
      "source": [
        "#libraries\n",
        "import re \n",
        "import spacy\n",
        "import en_core_web_sm\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('wordnet')\n",
        "from nltk.stem import WordNetLemmatizer \n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from io import StringIO\n",
        "!pip install pdfminer\n",
        "from pdfminer.pdfinterp import PDFResourceManager, PDFPageInterpreter\n",
        "from pdfminer.converter import TextConverter\n",
        "from pdfminer.layout import LAParams\n",
        "from pdfminer.pdfpage import PDFPage\n",
        "from wordcloud import WordCloud\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ezGanzb2EaO4"
      },
      "source": [
        "# 2. Parse resume"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bvKpTFTmEd6g"
      },
      "source": [
        "# function to parse resume text\n",
        "def resume_parser(fname, pages=None):\n",
        "    if not pages:\n",
        "        pagenums = set()\n",
        "    else:\n",
        "        pagenums = set(pages)\n",
        "    output = StringIO()\n",
        "    manager = PDFResourceManager()\n",
        "    converter = TextConverter(manager, output, laparams=LAParams())\n",
        "    interpreter = PDFPageInterpreter(manager, converter)\n",
        "    infile = open(fname, 'rb')\n",
        "    for page in PDFPage.get_pages(infile, pagenums):\n",
        "        interpreter.process_page(page)\n",
        "    infile.close()\n",
        "    converter.close()\n",
        "    text = output.getvalue()\n",
        "    output.close\n",
        "    return text\n",
        "\n",
        "# call the resume_parser function to extract text from pdf file\n",
        "resume_text = resume_parser(\"/content/Obianuju_Okafor_Resume.pdf\")"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Udry1eM7Kc8D"
      },
      "source": [
        "# 3. Extract skills"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DEE8QdHPKd-t"
      },
      "source": [
        "# function to extract skills from resume text and select the most relevant skills\n",
        "def extract_skills(resume_text):\n",
        "  \n",
        "    # remove punctuation\n",
        "    resume_text = re.sub('[-,()\\\\.!?]', '', resume_text)\n",
        "    \n",
        "    # turn to lowercase\n",
        "    resume_text = resume_text.lower()\n",
        "    \n",
        "    # Init the Wordnet Lemmatizer\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    \n",
        "    # tokenize resume text\n",
        "    tokens = nltk.word_tokenize(resume_text)\n",
        "\n",
        "    # Lemmatize tokens\n",
        "    tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
        "\n",
        "    # remove the tokens that are stop-words\n",
        "    stop_words = stopwords.words('english')\n",
        "\n",
        "    tokens = [token for token in tokens if token not in stop_words]\n",
        "  \n",
        "    # read the skills.csv file (skills bank)\n",
        "    data = pd.read_csv(\"skills.csv\") \n",
        "    \n",
        "    # create a list of all possible skills from our skills.csv file\n",
        "    skills = list(data.columns.values)\n",
        "   \n",
        "   # create an array to add the relevant skills after coparison\n",
        "    relevant_skills = []\n",
        "    \n",
        "    # compare one-grams (e.g python) skills from resume to skills from skills.csv, if there is a match add to relevant_skills array\n",
        "    for token in tokens:\n",
        "        if token.lower() in skills:\n",
        "            relevant_skills .append(token)\n",
        "\n",
        "    # load pre-trained model\n",
        "    nlp = spacy.load('en_core_web_sm')\n",
        "    nlp_text = nlp(resume_text)\n",
        "    # create bi-grams and tri-grams\n",
        "    noun_chunks = nlp_text.noun_chunks\n",
        "    \n",
        "    # compare bi-grams and tri-grams (e.g machine learning) skills from resume to skills from skills.csv, if there is a match add to relevant_skills array\n",
        "    for token in noun_chunks:\n",
        "        token = token.text.lower().strip()\n",
        "        if token in skills:\n",
        "            relevant_skills .append(token)\n",
        "    \n",
        "    return [i.capitalize() for i in set([i.lower() for i in relevant_skills])]\n",
        "\n",
        "# call the extract_skills function to select relevant skills from resume text\n",
        "relevant_skills = extract_skills(resume_text)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1chLUdeDMgB0"
      },
      "source": [
        "# 4. Create word cloud"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rjf2QPuoLzXF"
      },
      "source": [
        "# convert relevant_skills array to a single string\n",
        "relevant_skills_string = ''\n",
        "relevant_skills_string += \" \".join(relevant_skills)+\" \"\n",
        "# create and generate a word cloud image using relevant_skills_strings\n",
        "wordcloud = WordCloud(max_font_size=50, max_words=100, background_color=\"white\").generate(relevant_skills_string)\n",
        "# display the generated image\n",
        "plt.figure()\n",
        "plt.imshow(wordcloud, interpolation='bilinear')\n",
        "plt.axis(\"off\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G61F3dUnOiH7"
      },
      "source": [
        "# 5. Edit the word cloud shape to match your Linkedin header "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c8yJ7wAzOyzd"
      },
      "source": [
        "# create mask using Linkedin header template\n",
        "linkedin_header_mask = np.array(Image.open(\"/content/linkedin_header_template.png\"))\n",
        "\n",
        "# create and generate a word cloud image using mask\n",
        "wc = WordCloud(background_color=\"white\", max_words=1000, mask=linkedin_header_mask,\n",
        "               contour_width=3, contour_color='white').generate(relevant_skills_string)\n",
        "\n",
        "# display the generated image\n",
        "plt.figure(figsize=[20,10])\n",
        "plt.imshow(wc, interpolation='bilinear')\n",
        "plt.axis(\"off\")\n",
        "plt.show()\n",
        "\n",
        "# save generated wordcloud image as linkedin_wordcloud_header\n",
        "wc.to_file(\"linkedin_wordcloud_header.png\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}